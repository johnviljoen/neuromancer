{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antiderivative Operator - Aligned Dataset\n",
    "\n",
    "This tutorial demonstrates the use of learning neural operators for a data driven use case (non-physics informed). \n",
    "\n",
    "### References\n",
    "[1] [Antiderivative operator from an aligned dataset - DeepXDE](https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html)\n",
    "\n",
    "[2] [DeepONet Tutorial in JAX](https://github.com/Ceyron/machine-learning-and-simulation/blob/main/english/neural_operators/simple_deepOnet_in_JAX.ipynb)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Added this to get working in JetBrains DataSpell, may work without in VSCode\n",
    "import os\n",
    "\n",
    "current = os.path.abspath('')\n",
    "print(current)\n",
    "repo_name = 'neuromancer_deeponet'\n",
    "while current[-(len(repo_name)):] != repo_name:\n",
    "    parent = os.path.dirname(current)\n",
    "    print(parent)\n",
    "    os.chdir(parent)\n",
    "    current = parent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#%pip install \"neuromancer[examples] @ git+https://github.com/pnnl/neuromancer.git@master\"\n",
    "#%pip install watermark"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# FIXME only for develoment\n",
    "import sys\n",
    "sys.path.append('src')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from neuromancer.callbacks import Callback\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules.blocks import MLP\n",
    "from neuromancer.modules.activations import activations\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.dynamics.operators import DeepONet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PyTorch random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# numpy random seed\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "original source: [https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html](https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html)  \n",
    "\n",
    "We will learn the antiderivative operator \n",
    "\n",
    "$$G : v \\mapsto u$$\n",
    "\n",
    "defined by an ODE\n",
    "\n",
    "$$\\frac{du(x)}{dx} = v(x),\\;\\;x\\in [0,1]$$\n",
    "\n",
    "**Initial Condition:** \n",
    "$$u(0) = (0)$$\n",
    "\n",
    "We learn *G* from a dataset. Each data point in the dataset is one pair of (v,u), generated as follows:\n",
    "\n",
    "1. A random function *v* is sampled from a Gaussian random field (GRF) with the resolution m = 100.\n",
    "2. Solve *u* for *v* numerically. We assume that for each *u*, we have the values of *u(x)* in the same N<sub>u</sub> = 100 locations. Because we have the values of *u(x)* in the same locations, we call this dataset as \"aligned data\".\n",
    "\n",
    "* Dataset information\n",
    "    * The training dataset has size 150.\n",
    "    * The testing dataset has size 1000. (We split this into a dev/test split of size 500 each)\n",
    "    * Input of the branch net: the functions *v*. It is a matrix of shape (dataset size, m), e.g., (150, 100) for the training dataset.\n",
    "    * Input of the trunk net: the locations *x* of *u(x)*. It is a matrix of shape (*N<sub>u</sub>*, dimension)\n",
    "        * i.e., (100,1) for both training and testing datasets.\n",
    "    * Output: The values of *u(x)* in different locations for different *v*. It is a matrix of shape (dataset size, *N<sub>u</sub>*).\n",
    "        * e.g., (150, 100) for the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_dir = \"examples/neural_operators/datasets\"\n",
    "Path(data_dir).mkdir(exist_ok=True, parents=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#!wget https://github.com/pnnl/neuromancer/raw/master/examples/DeepONets/datasets/antiderivative_aligned_test.npz -nc -O examples/neural_operators/datasets/antiderivative_aligned_test.npz\n",
    "#!wget https://github.com/pnnl/neuromancer/raw/master/examples/DeepONets/datasets/antiderivative_aligned_train.npz -nc -O examples/neural_operators/datasets/antiderivative_aligned_train.npz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def prepare_data(dataset, name):\n",
    "    ## Note: transposing branch input because DictDataset in Neuromancer needs all tensors in the dict to have the same shape at index 0\n",
    "    branch_inputs = dataset[\"X\"][0].T\n",
    "    trunk_inputs = dataset[\"X\"][1]\n",
    "    outputs = dataset[\"y\"].T\n",
    "\n",
    "    Nu = outputs.shape[0]\n",
    "    Nsamples = outputs.shape[1]\n",
    "    print(f'{name} dataset: Nu = {Nu}, Nsamples = {Nsamples}')\n",
    "\n",
    "    # convert to pytorch tensors of float type\n",
    "    t_branch_inputs = torch.from_numpy(branch_inputs).float()\n",
    "    t_trunk_inputs = torch.from_numpy(trunk_inputs).float()\n",
    "    t_outputs = torch.from_numpy(outputs).float()\n",
    "\n",
    "    data = DictDataset({\n",
    "        \"branch_inputs\": t_branch_inputs,\n",
    "        \"trunk_inputs\": t_trunk_inputs,\n",
    "        \"outputs\": t_outputs\n",
    "    }, name=name)\n",
    "\n",
    "    return data, Nu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def split_test_into_dev_test(original_test):\n",
    "    dataset_dev = dict()\n",
    "    dataset_dev['X'] = dict()\n",
    "    dataset_test = dict()\n",
    "    dataset_test['X'] = dict()\n",
    "    # split original test into dev and test\n",
    "    dev_branch_inputs, test_branch_inputs = np.vsplit(original_test['X'][0], 2)\n",
    "    dev_trunk_inputs, test_trunk_inputs = (original_test['X'][1], original_test['X'][1])\n",
    "    dataset_dev['X'] = (dev_branch_inputs, dev_trunk_inputs)\n",
    "    dataset_test['X'] = (test_branch_inputs, test_trunk_inputs)\n",
    "    dataset_dev['y'], dataset_test['y'] = np.vsplit(original_test['y'], 2)\n",
    "    return dataset_dev, dataset_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create named dictionary datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_dir = \"examples/neural_operators/datasets\"\n",
    "\n",
    "# Load original train/testsplit files\n",
    "dataset_train = np.load(f\"{data_dir}/antiderivative_aligned_train.npz\", allow_pickle=True)\n",
    "original_test = np.load(f\"{data_dir}/antiderivative_aligned_test.npz\", allow_pickle=True)\n",
    "\n",
    "# Split original test set into 50/50 dev/test splits\n",
    "dataset_dev, dataset_test = split_test_into_dev_test(original_test)\n",
    "\n",
    "# Prepare data by transforming shapes\n",
    "train_data, Nu_train = prepare_data(dataset_train, name=\"train\")\n",
    "dev_data, Nu_dev = prepare_data(dataset_dev, name=\"dev\")\n",
    "test_data, Nu_test = prepare_data(dataset_test, name=\"test\")\n",
    "\n",
    "# set Nu to one of the values from the splits after verifying they are the same\n",
    "Nu = Nu_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch DataLoaders for the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = 100\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=train_data.collate_fn, shuffle=False)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, collate_fn=dev_data.collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=test_data.collate_fn, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define node"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "in_size_branch = Nu\n",
    "width_size = 40\n",
    "depth_branch = 2\n",
    "interact_size = 40\n",
    "in_size_trunk = 1\n",
    "depth_trunk = 2\n",
    "branch_net = MLP(\n",
    "    insize=in_size_branch,\n",
    "    outsize=interact_size,\n",
    "    nonlin=nn.ReLU,\n",
    "    hsizes=[width_size] * depth_branch,\n",
    "    bias=True,\n",
    ")\n",
    "trunk_net = MLP(\n",
    "    insize=in_size_trunk,\n",
    "    outsize=interact_size,\n",
    "    nonlin=nn.ReLU,\n",
    "    hsizes=[width_size] * depth_trunk,\n",
    "    bias=True,\n",
    ")\n",
    "deeponet = DeepONet(\n",
    "    branch_net=branch_net,\n",
    "    trunk_net=trunk_net,\n",
    "    bias=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "node_deeponet = Node(deeponet, ['branch_inputs', 'trunk_inputs'], ['g'], name=\"deeponet\")\n",
    "print(node_deeponet)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Constraints in NeuroMANCER\n",
    "\n",
    "We use Mean Squared Error(MSE) for our loss function\n",
    "\n",
    "$$\\sum_{i=1}^{D}(x_i-y_i)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "var_y_est = variable(\"g\")\n",
    "var_y_true = variable(\"outputs\")\n",
    "\n",
    "nodes = [node_deeponet]\n",
    "\n",
    "var_loss = (var_y_est == var_y_true.T)^2\n",
    "var_loss.name = \"residual_loss\"\n",
    "objectives = [var_loss]\n",
    "\n",
    "loss = PenaltyLoss(objectives, constraints=[])\n",
    "\n",
    "problem = Problem(nodes, loss=loss, grad_inference=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Solution in NeuroMANCER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lr = 0.001              # step size for gradient descent\n",
    "epochs = 10000          # number of training epochs\n",
    "epoch_verbose = 100     # print loss/display loss plot when this many epochs have occurred\n",
    "warmup = 100            # number of epochs to wait before enacting early stopping policy\n",
    "patience = 0            # number of epochs with no improvement in eval metric to allow before early stopping"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Trainer and solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "    def end_epoch(self, trainer, output):\n",
    "        if trainer.current_epoch % trainer.epoch_verbose == 0:\n",
    "            train_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"train\"]]\n",
    "            dev_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"dev\"]]\n",
    "            clear_output(wait=True)\n",
    "            plt.semilogy(train_loss_history, label=\"Train loss\")\n",
    "            plt.semilogy(dev_loss_history, label=\"Dev loss\")\n",
    "            plt.xlabel(\"# Epochs\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    problem.to(device),\n",
    "    train_data=train_loader,\n",
    "    dev_data=dev_loader,\n",
    "    test_data=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    logger=None,\n",
    "    callback=loss_history_callback,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    epoch_verbose=epoch_verbose,\n",
    "    train_metric='train_loss',\n",
    "    dev_metric='dev_loss',\n",
    "    test_metric='test_loss',\n",
    "    eval_metric=\"dev_loss\",\n",
    "    warmup = warmup,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "best_model = trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load best trained model\n",
    "best_outputs = trainer.test(best_model)\n",
    "problem.load_state_dict(best_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"train\"]]\n",
    "dev_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"dev\"]]\n",
    "mean_test_loss = best_outputs['mean_test_loss'].detach().cpu().numpy()\n",
    "print(mean_test_loss)\n",
    "print(f\"len(train_loss_history): {len(train_loss_history)}\")\n",
    "print(f\"len(dev_loss_history): {len(dev_loss_history)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss history w/ mean test loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.semilogy(train_loss_history, label=\"Train loss\")\n",
    "plt.semilogy(dev_loss_history, label=\"Dev loss\")\n",
    "plt.scatter(len(train_loss_history), mean_test_loss, label=\"Mean test loss\", c=\"red\", marker='x')\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "k = 18 # k is the k-th function among the 500 test functions\n",
    "v_ = test_data.datadict[\"branch_inputs\"][:,k].reshape(-1,1)\n",
    "x_ = test_data.datadict[\"trunk_inputs\"]\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = test_data.datadict[\"outputs\"][:,k]\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='v_')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='u_')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='u_est')\n",
    "\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Integral Examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_ = train_data.datadict[\"trunk_inputs\"]\n",
    "v_ = torch.pow(x_,2).reshape(-1,1)\n",
    "\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = (1./3.)*torch.pow(x_,3).reshape(-1,1)\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='$v(x) = x^2$')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='integral of v, exact ($x^3/3$)')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='integral of v, estimated')\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_ = train_data.datadict[\"trunk_inputs\"]\n",
    "v_ = torch.cos(x_).reshape(-1,1)\n",
    "\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = torch.sin(x_).reshape(-1,1)\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='$v(x) = cos(x)$')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='integral of v, exact ($sin(x)$)')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='integral of v, estimated')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eerc-deeponet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
