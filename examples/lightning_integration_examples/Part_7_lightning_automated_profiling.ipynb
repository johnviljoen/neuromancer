{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Profiling In PyTorch Lightning \n",
    "\n",
    "This example (which utilizes the same problem set-up as in \"complex_objectve_function_example.py) shows how one can easily use a Torch Profiler with PyTorch Lightning to view bottlenecks and performance benchmarking. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import neuromancer as nm\n",
    "from neuromancer.dataset import DictDataset\n",
    "\n",
    "import lightning.pytorch as pl \n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from neuromancer.trainer import Trainer, LitTrainer\n",
    "from neuromancer.problem import Problem, LitProblem\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset, LitDataModule\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem and Data Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Returns:\n",
      "[0.00705232 0.00291352 0.00902834 0.00806446 0.00449609]\n",
      "Covariance Matrix:\n",
      "[[0.0023143  0.00113818 0.00231431 0.00112589 0.00155372]\n",
      " [0.00113818 0.00083598 0.00163794 0.00078066 0.00101279]\n",
      " [0.00231431 0.00163794 0.00333726 0.00137955 0.00197275]\n",
      " [0.00112589 0.00078066 0.00137955 0.00094245 0.00101531]\n",
      " [0.00155372 0.00101279 0.00197275 0.00101531 0.00162734]]\n"
     ]
    }
   ],
   "source": [
    "# Define the data setup function\n",
    "def data_setup_function(exp_returns): \n",
    "        p_low, p_high = max(min(exp_returns),0), max(exp_returns)\n",
    "        data_train = DictDataset({\"p\": torch.FloatTensor(1000, 1).uniform_(p_low, p_high)}, name='train')\n",
    "        data_test = DictDataset({\"p\": torch.FloatTensor(100, 1).uniform_(p_low, p_high)}, name='test')\n",
    "        data_dev = DictDataset({\"p\": torch.FloatTensor(100, 1).uniform_(p_low, p_high)}, name='dev')\n",
    "        return data_train, data_dev, data_test, 32\n",
    "\n",
    "\n",
    "\n",
    "num_vars = 5 #set to 2 for performance reasons\n",
    "\n",
    "# expected returns\n",
    "exp_returns = np.random.uniform(0.002, 0.01, num_vars)\n",
    "print(\"Expected Returns:\")\n",
    "print(exp_returns)\n",
    "\n",
    "# covariance matrix\n",
    "A = np.random.rand(num_vars, num_vars)\n",
    "# positive semi-definite matrix\n",
    "cov_matrix = A @ A.T / 1000\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "\n",
    "# parameters\n",
    "p = nm.constraint.variable(\"p\")\n",
    "# variables\n",
    "x = nm.constraint.variable(\"x\")\n",
    "\n",
    "# objective function\n",
    "f = sum(cov_matrix[i, j] * x[:, i] * x[:, j] for i in range(num_vars) for j in range(num_vars))\n",
    "obj = [f.minimize(weight=1.0, name=\"obj\")]\n",
    "\n",
    "# constraints\n",
    "constrs = []\n",
    "# constr: 100 units\n",
    "con = 100 * (sum(x[:, i] for i in range(num_vars)) == 1)\n",
    "con.name = \"c_units\"\n",
    "constrs.append(con)\n",
    "# constr: expected return\n",
    "con = 100 * (sum(exp_returns[i] * x[:, i] for i in range(num_vars)) >= p[:, 0])\n",
    "con.name = \"c_return\"\n",
    "constrs.append(con)\n",
    "\n",
    "# define neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=1, outsize=num_vars, bias=True,\n",
    "                        linear_map=nm.slim.maps[\"linear\"], nonlin=nn.ReLU, hsizes=[5]*4)\n",
    "# solution map from model parameters: sol_map(p) -> x\n",
    "sol_map = nm.system.Node(func, [\"p\"], [\"x\"], name=\"smap\")\n",
    "# trainable components\n",
    "components = [sol_map]\n",
    "\n",
    "# merit loss function\n",
    "loss = nm.loss.PenaltyLoss(obj, constrs)\n",
    "# problem\n",
    "problem = nm.problem.Problem(components, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Lightning and Profile\n",
    "\n",
    "We will use the \"simple\" profiler for viewing end-to-end runtime. For bottleneck analysis, the recommendation is to use \"pytorch\" profiler. For more profiling options please refer to: \n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/1.5.10/advanced/profiler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_returns': array([0.00705232, 0.00291352, 0.00902834, 0.00806446, 0.00449609])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.001  # step size for gradient descent\n",
    "\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define lightning data module\n",
    "lit_data_module = LitDataModule(data_setup_function=data_setup_function, exp_returns=exp_returns)\n",
    "\n",
    "# Define lightning trainer. We use GPU acceleration utilizing 2 GPUS. We tell Lightning to \n",
    "# distribute training parallely (strategy=ddp)\n",
    "lit_trainer = LitTrainer(epochs=1, accelerator=\"cpu\", dev_metric='dev_loss', profiler='pytorch')\n",
    "\n",
    "# Train problem to the data module\n",
    "best_problem_weights = lit_trainer.fit(problem, lit_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromancer3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
