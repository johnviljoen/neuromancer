{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34qVD_ntSKLF"
   },
   "source": [
    "# Custom Training Logic with Lightning Integration\n",
    "\n",
    "In this example, we showcase the ability for the user to define own training logic and easily integrate into Lightning workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCn3zpaIqgMc"
   },
   "source": [
    "## NeuroMANCER and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzy5Wot5k2Gf"
   },
   "source": [
    "### Install (Colab only)\n",
    "Skip this step when running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X_3EvkSz0Fnz",
    "outputId": "23c06f6b-ab48-4763-c43c-40a325cacf87"
   },
   "outputs": [],
   "source": [
    "!pip install \"neuromancer[examples] @ git+https://github.com/pnnl/neuromancer.git@master\"\n",
    "!pip install lightning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWyvndXlz0Fv"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The user might need to install PyTorch Lightning). If so, please run \n",
    "\n",
    "```\n",
    "pip install lightning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KbP0n-4evRqt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import neuromancer.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as patheffects\n",
    "import casadi\n",
    "import time\n",
    "import lightning.pytorch as pl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "POL27EJZxJmI"
   },
   "outputs": [],
   "source": [
    "from neuromancer.trainer import Trainer, LitTrainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem formulation\n",
    "\n",
    "In this example we will solve parametric constrained [Rosenbrock problem](https://en.wikipedia.org/wiki/Rosenbrock_function):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{minimize } &&  (1-x)^2 + a(y-x^2)^2\\\\\n",
    "&\\text{subject to} && \\left(\\frac{p}{2}\\right)^2 \\le x^2 + y^2 \\le p^2\\\\\n",
    "& && x \\ge y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with parameters $p, a$ and decision variables $x, y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Workflow\n",
    "\n",
    "The workflow when using Lightning consists of three parts: \n",
    "\n",
    "1. Defining a \"data_setup_function() -- this function should return 4 values (train, dev, test datasets, and batch size). The datasets should be named Neuromancer DictDatasets. \n",
    "2. Defining the Problem -- consisting of Nodes, System, Loss. \n",
    "3. Instantiating the PyTorch-Lightning -based Trainer (LitTrainer class)\n",
    "\n",
    "For this notebook, we assume all operations are done on the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WH7o7Wu1epw"
   },
   "source": [
    "### Lightning Dataset\n",
    "\n",
    "We constructy the dataset by sampling the parametric space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_r6p2p6myHAh"
   },
   "outputs": [],
   "source": [
    "data_seed = 408  # random seed used for simulated data\n",
    "np.random.seed(data_seed)\n",
    "torch.manual_seed(data_seed)\n",
    "nsim = 5000  # number of datapoints: increase sample density for more robust results\n",
    "\n",
    "# create dictionaries with sampled datapoints with uniform distribution\n",
    "a_low, a_high, p_low, p_high = 0.2, 1.2, 0.5, 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ9qrw0tlJhs"
   },
   "source": [
    "We define the **data_setup_function()** below. It randomly sample parameters from a uniform distribution: $0.5\\le p\\le2.0$;  $0.2\\le a\\le1.2$. It takes these parameters as inputs and outputs Neuromancer DictDatasets() for train, dev, and test data (or None type otherwise), as well as batch size. We have hardcoded batch size to be 64 in this case. \n",
    "\n",
    "It is important to define both training and dev/validation datasets. Training datasets will be used for the training step; dev datasets will be used for model checkpointing (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nu58M-8JyHy6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_setup_function(nsim, a_low, a_high, p_low, p_high): \n",
    "\n",
    "    \n",
    "    samples_train = {\"a\": torch.FloatTensor(nsim, 1).uniform_(a_low, a_high),\n",
    "                    \"p\": torch.FloatTensor(nsim, 1).uniform_(p_low, p_high)}\n",
    "    samples_dev = {\"a\": torch.FloatTensor(nsim, 1).uniform_(a_low, a_high),\n",
    "                \"p\": torch.FloatTensor(nsim, 1).uniform_(p_low, p_high)}\n",
    "    samples_test = {\"a\": torch.FloatTensor(nsim, 1).uniform_(a_low, a_high),\n",
    "                \"p\": torch.FloatTensor(nsim, 1).uniform_(p_low, p_high)}\n",
    "    # create named dictionary datasets\n",
    "    train_data = DictDataset(samples_train, name='train')\n",
    "    dev_data = DictDataset(samples_dev, name='dev')\n",
    "    test_data = DictDataset(samples_test, name='test')\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    # Return the dict datasets in train, dev, test order, followed by batch_size \n",
    "    return train_data, dev_data, test_data, batch_size \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the **Problem()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2htUaWMDjsk"
   },
   "source": [
    "## Primal Solution Map Architecture\n",
    "\n",
    "A neural network mapping problem parameters onto primal decision variables:  \n",
    "$$x = \\pi(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ta_I_pjyyLzf"
   },
   "outputs": [],
   "source": [
    "# define neural architecture for the trainable solution map\n",
    "func = blocks.MLP(insize=2, outsize=2,\n",
    "                bias=True,\n",
    "                linear_map=slim.maps['linear'],\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[80] * 4)\n",
    "# wrap neural net into symbolic representation of the solution map via the Node class: sol_map(xi) -> x\n",
    "sol_map = Node(func, ['a', 'p'], ['x'], name='map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxj77EFj7EO-"
   },
   "source": [
    "## Objective and Constraints in NeuroMANCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bcoVjphjyPp9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "variable is a basic symbolic abstraction in Neuromancer\n",
    "   x = variable(\"variable_name\")                      (instantiates new variable)  \n",
    "variable construction supports:\n",
    "   algebraic expressions:     x**2 + x**3 + 5     (instantiates new variable)  \n",
    "   slicing:                   x[:, i]             (instantiates new variable)  \n",
    "   pytorch callables:         torch.sin(x)        (instantiates new variable)  \n",
    "   constraints definition:    x <= 1.0            (instantiates Constraint object) \n",
    "   objective definition:      x.minimize()        (instantiates Objective object) \n",
    "to visualize computational graph of the variable use x.show() method          \n",
    "\"\"\"\n",
    "\n",
    "# define decision variables\n",
    "x1 = variable(\"x\")[:, [0]]\n",
    "x2 = variable(\"x\")[:, [1]]\n",
    "# problem parameters sampled in the dataset\n",
    "p = variable('p')\n",
    "a = variable('a')\n",
    "\n",
    "# objective function\n",
    "f = (1-x1)**2 + a*(x2-x1**2)**2\n",
    "obj = f.minimize(weight=1.0, name='obj')\n",
    "\n",
    "# constraints\n",
    "Q_con = 100.  # constraint penalty weights\n",
    "con_1 = Q_con*(x1 >= x2)\n",
    "con_2 = Q_con*((p/2)**2 <= x1**2+x2**2)\n",
    "con_3 = Q_con*(x1**2+x2**2 <= p**2)\n",
    "con_1.name = 'c1'\n",
    "con_2.name = 'c2'\n",
    "con_3.name = 'c3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "n7VPa9Wc8JRB",
    "outputId": "0da17c45-6370-4f46-f626-bd5686b94bfc"
   },
   "outputs": [],
   "source": [
    "# constrained optimization problem construction\n",
    "objectives = [obj]\n",
    "constraints = [con_1, con_2, con_3]\n",
    "components = [sol_map]\n",
    "\n",
    "# create penalty method loss function\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(components, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Logic\n",
    "Training within PyTorch Lightning framework is defined by a `training_step` function, which defines the logic going from a data batch to loss. For example, the default training_step used is shown below (other extraneous details removed for simplicity). Here, we get the problem output for the given batch and return the loss associated with that output.\n",
    "\n",
    "```\n",
    "def training_step(self, batch):\n",
    "    output = self.problem(batch)\n",
    "    loss = output[self.train_metric]\n",
    "    return loss\n",
    "```\n",
    "While rare, there may be instances where the user might want to define their own training logic. Potential cases include test-time data augmentation (e.g. operations on/w.r.t the data rollout), other domain augmentations, or modifications to how the output and/or loss is handled. \n",
    "\n",
    "The user can pass in their own \"training_step\" by supplying an equivalent function handler to the \"custom_training_step\" keyword of LitTrainer, for example: \n",
    "\n",
    "```\n",
    "def custom_training_step(model, batch): \n",
    "    output = model.problem(batch)\n",
    "    Q_con = 1\n",
    "    if model.current_epoch > 1: \n",
    "        Q_con = 1/10000\n",
    "    loss = Q_con*(output[model.train_metric])\n",
    "    return loss\n",
    "```\n",
    "\n",
    "The signature of this function should be `custom_training_step(model, batch)` where model is a Neuromancer Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/birm560/neuromancer/examples/lightning_integration_examples/other_examples/lightning_logs\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ./ exists and is not empty.\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | problem | Problem | 19.8 K\n",
      "------------------------------------\n",
      "19.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.8 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=159` in the `DataLoader` to improve performance.\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=159` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:00<00:00, 121.77it/s, v_num=0, train_loss_step=0.903]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:01<00:00, 73.36it/s, v_num=0, train_loss_step=0.903, dev_loss=0.815, train_loss_epoch=5.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 79: 'dev_loss' reached 0.81470 (best 0.81470), saving model to './epoch=0-step=79.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 40/79 [00:00<00:00, 121.57it/s, v_num=0, train_loss_step=0.714, dev_loss=0.815, train_loss_epoch=5.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "def custom_training_step(model, batch): \n",
    "    output = model.problem(batch)\n",
    "    Q_con = 1\n",
    "    if model.current_epoch > 1: \n",
    "        Q_con = 1/10000    \n",
    "    loss = Q_con*(output[model.train_metric])\n",
    "    return loss\n",
    "\n",
    "lit_trainer = LitTrainer(epochs=100, accelerator='cpu', patience=3, custom_training_step=custom_training_step)\n",
    "lit_trainer.fit(problem=problem, data_setup_function=data_setup_function, nsim=nsim,a_low=0.2, a_high=1.2, p_low=0.5, p_high=2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another example of a dummy custom_training_step. Here we want to add the loss of the previous batch and accumulate into the \"current\" loss. (Again this is a dummy example and not necessarily propel ML techniques). Any sort of variables, such as \"past_loss\" can be defined by setting them as attributes of \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ./ exists and is not empty.\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | problem | Problem | 19.8 K\n",
      "------------------------------------\n",
      "19.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.8 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=159` in the `DataLoader` to improve performance.\n",
      "/home/birm560/miniconda3/envs/neuromancer3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=159` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:01<00:00, 75.73it/s, v_num=3, train_loss_step=0.0857, dev_loss=0.0834, train_loss_epoch=0.137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 79: 'dev_loss' reached 0.08340 (best 0.08340), saving model to './epoch=0-step=79-v3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 79/79 [00:01<00:00, 74.35it/s, v_num=3, train_loss_step=0.213, dev_loss=0.0887, train_loss_epoch=0.282] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 158: 'dev_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 79/79 [00:01<00:00, 75.86it/s, v_num=3, train_loss_step=0.309, dev_loss=0.138, train_loss_epoch=0.284]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 237: 'dev_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 79/79 [00:01<00:00, 75.04it/s, v_num=3, train_loss_step=0.206, dev_loss=0.0908, train_loss_epoch=0.243]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 316: 'dev_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 79/79 [00:01<00:00, 73.78it/s, v_num=3, train_loss_step=0.206, dev_loss=0.0908, train_loss_epoch=0.243]\n"
     ]
    }
   ],
   "source": [
    "def custom_training_step(model, batch): \n",
    "    with torch.no_grad(): \n",
    "        if model.current_epoch == 0: \n",
    "            model.past_loss = 0\n",
    "    \n",
    "    output = model.problem(batch)\n",
    "    loss = (output[model.train_metric]) + 0.5*model.past_loss\n",
    "    model.past_loss = loss.item()\n",
    "    return loss\n",
    "\n",
    "lit_trainer = LitTrainer(epochs=100, accelerator='cpu', patience=3, custom_training_step=custom_training_step)\n",
    "lit_trainer.fit(problem=problem, data_setup_function=data_setup_function, nsim=nsim,a_low=0.2, a_high=1.2, p_low=0.5, p_high=2.0)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuromancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
