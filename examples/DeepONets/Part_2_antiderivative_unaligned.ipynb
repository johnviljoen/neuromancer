{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antiderivative Operator - Aligned Dataset\n",
    "\n",
    "This tutorial demonstrates the use of learning neural operators for a data driven use case (non-physics informed). \n",
    "\n",
    "### References\n",
    "[1] [Antiderivative operator from an aligned dataset - DeepXDE](https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html)\n",
    "\n",
    "[2] [DeepONet Tutorial in JAX](https://github.com/Ceyron/machine-learning-and-simulation/blob/main/english/neural_operators/simple_deepOnet_in_JAX.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:52.835248Z",
     "start_time": "2024-06-19T23:16:52.832364Z"
    }
   },
   "source": [
    "#%pip install \"neuromancer[examples] @ git+https://github.com/pnnl/neuromancer.git@master\"\n",
    "#%pip install watermark"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:52.883011Z",
     "start_time": "2024-06-19T23:16:52.878390Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hamc649/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet/examples/DeepONets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:52.886482Z",
     "start_time": "2024-06-19T23:16:52.883957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Added this to get working in JetBrains DataSpell, may work without in VSCode\n",
    "import os\n",
    "\n",
    "current = os.path.abspath('')\n",
    "print(current)\n",
    "repo_name = \"neuromancer_deeponet\"\n",
    "while current[-(len(repo_name)):] != repo_name:\n",
    "    parent = os.path.dirname(current)\n",
    "    print(parent)\n",
    "    os.chdir(parent)\n",
    "    current = parent"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hamc649/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet/examples/DeepONets\n",
      "/Users/hamc649/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet/examples\n",
      "/Users/hamc649/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:56.989114Z",
     "start_time": "2024-06-19T23:16:52.887529Z"
    }
   },
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from examples.DeepONets.helper import prepare_data, split_test_into_dev_test\n",
    "\n",
    "# only for development\n",
    "import sys\n",
    "sys.path.append('src')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:56.992830Z",
     "start_time": "2024-06-19T23:16:56.990384Z"
    }
   },
   "source": [
    "from neuromancer.callbacks import Callback\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.modules.activations import activations\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.trainer import Trainer"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:57.024636Z",
     "start_time": "2024-06-19T23:16:56.993614Z"
    }
   },
   "source": [
    "# PyTorch random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# numpy random seed\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "original source: [https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html](https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_aligned.html)  \n",
    "\n",
    "We will learn the antiderivative operator \n",
    "\n",
    "$$G : v \\mapsto u$$\n",
    "\n",
    "defined by an ODE\n",
    "\n",
    "$$\\frac{du(x)}{dx} = v(x),\\;\\;x\\in [0,1]$$\n",
    "\n",
    "**Initial Condition:** \n",
    "$$u(0) = (0)$$\n",
    "\n",
    "We learn *G* from a dataset. Each data point in the dataset is one pair of (v,u), generated as follows:\n",
    "\n",
    "1. A random function *v* is sampled from a Gaussian random field (GRF) with the resolution m = 100.\n",
    "2. Solve *u* for *v* numerically. We assume that for each *u*, we have the values of *u(x)* in the same N<sub>u</sub> = 100 locations. Because we have the values of *u(x)* in the same locations, we call this dataset as \"aligned data\".\n",
    "\n",
    "* Dataset information\n",
    "    * The training dataset has size 150.\n",
    "    * The testing dataset has size 1000. (We split this into a dev/test split of size 500 each)\n",
    "    * Input of the branch net: the functions *v*. It is a matrix of shape (dataset size, m), e.g., (150, 100) for the training dataset.\n",
    "    * Input of the trunk net: the locations *x* of *u(x)*. It is a matrix of shape (*N<sub>u</sub>*, dimension)\n",
    "        * i.e., (100,1) for both training and testing datasets.\n",
    "    * Output: The values of *u(x)* in different locations for different *v*. It is a matrix of shape (dataset size, *N<sub>u</sub>*).\n",
    "        * e.g., (150, 100) for the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:57.027273Z",
     "start_time": "2024-06-19T23:16:57.025187Z"
    }
   },
   "source": [
    "data_dir = \"examples/DeepONets/datasets\"\n",
    "Path(data_dir).mkdir(exist_ok=True, parents=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:16:57.029703Z",
     "start_time": "2024-06-19T23:16:57.028123Z"
    }
   },
   "source": [
    "#!wget https://github.com/pnnl/neuromancer/raw/master/examples/DeepONets/datasets/antiderivative_unaligned_test.npz -nc -O examples/DeepONets/datasets/antiderivative_aligned_test.npz\n",
    "#!wget https://github.com/pnnl/neuromancer/raw/master/examples/DeepONets/datasets/antiderivative_unaligned_train.npz -nc -O examples/DeepONets/datasets/antiderivative_aligned_train.npz"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create named dictionary datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:20:32.598787Z",
     "start_time": "2024-06-19T23:20:32.224892Z"
    }
   },
   "source": [
    "data_dir = \"examples/DeepONets/datasets\"\n",
    "\n",
    "# Load original train/testsplit files\n",
    "dataset_train = np.load(f\"{data_dir}/antiderivative_unaligned_train.npz\", allow_pickle=True)\n",
    "original_test = np.load(f\"{data_dir}/antiderivative_unaligned_test.npz\", allow_pickle=True)\n",
    "\n",
    "# Split original test set into 50/50 dev/test splits\n",
    "dataset_dev, dataset_test = split_test_into_dev_test(original_test)\n",
    "\n",
    "# Prepare data by transforming shapes\n",
    "train_data, Nu_train = prepare_data(dataset_train, name=\"train\")\n",
    "dev_data, Nu_dev = prepare_data(dataset_dev, name=\"dev\")\n",
    "test_data, Nu_test = prepare_data(dataset_test, name=\"test\")\n",
    "\n",
    "# set Nu to one of the values from the splits after verifying they are the same\n",
    "Nu = Nu_train"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: Nu = 1, Nsamples = 10000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatched number of samples in dataset tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m dataset_dev, dataset_test \u001B[38;5;241m=\u001B[39m split_test_into_dev_test(original_test)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Prepare data by transforming shapes\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m train_data, Nu_train \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m dev_data, Nu_dev \u001B[38;5;241m=\u001B[39m prepare_data(dataset_dev, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m test_data, Nu_test \u001B[38;5;241m=\u001B[39m prepare_data(dataset_test, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet/examples/DeepONets/helper.py:19\u001B[0m, in \u001B[0;36mprepare_data\u001B[0;34m(dataset, name)\u001B[0m\n\u001B[1;32m     16\u001B[0m t_trunk_inputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(trunk_inputs)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     17\u001B[0m t_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(outputs)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 19\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mDictDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbranch_inputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_branch_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrunk_inputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_trunk_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_outputs\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data, Nu\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-PNNL/Documents/Neuromancer/repos/neuromancer_deeponet/src/neuromancer/dataset.py:102\u001B[0m, in \u001B[0;36mDictDataset.__init__\u001B[0;34m(self, datadict, name)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatadict \u001B[38;5;241m=\u001B[39m datadict\n\u001B[1;32m    101\u001B[0m lens \u001B[38;5;241m=\u001B[39m [v\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m datadict\u001B[38;5;241m.\u001B[39mvalues()]\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(lens)) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMismatched number of samples in dataset tensors\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;241m=\u001B[39m lens[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n",
      "\u001B[0;31mAssertionError\u001B[0m: Mismatched number of samples in dataset tensors"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch DataLoaders for the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = 100\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=train_data.collate_fn, shuffle=False)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, collate_fn=dev_data.collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=test_data.collate_fn, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define node"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "in_size_branch = Nu\n",
    "width_size = 40\n",
    "depth_branch = 2\n",
    "interact_size = 40\n",
    "in_size_trunk = 1\n",
    "depth_trunk = 2\n",
    "block_deeponet = blocks.DeepONet(\n",
    "    insize_branch=in_size_branch,\n",
    "    insize_trunk=in_size_trunk,\n",
    "    widthsize=width_size,\n",
    "    interactsize=interact_size,\n",
    "    depth_branch=depth_branch,\n",
    "    depth_trunk=depth_trunk,\n",
    "    nonlin=activations['relu'],\n",
    "    bias=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "node_deeponet = Node(block_deeponet, ['branch_inputs', 'trunk_inputs'], ['g'], name=\"deeponet\")\n",
    "print(node_deeponet)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Constraints in NeuroMANCER\n",
    "\n",
    "We use Mean Squared Error(MSE) for our loss function\n",
    "\n",
    "$$\\sum_{i=1}^{D}(x_i-y_i)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "var_y_est = variable(\"g\")\n",
    "var_y_true = variable(\"outputs\")\n",
    "\n",
    "nodes = [node_deeponet]\n",
    "\n",
    "var_loss = (var_y_est == var_y_true.T)^2\n",
    "var_loss.name = \"residual_loss\"\n",
    "objectives = [var_loss]\n",
    "\n",
    "loss = PenaltyLoss(objectives, constraints=[])\n",
    "\n",
    "problem = Problem(nodes, loss=loss, grad_inference=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Solution in NeuroMANCER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lr = 0.001              # step size for gradient descent\n",
    "epochs = 10000          # number of training epochs\n",
    "epoch_verbose = 100     # print loss/display loss plot when this many epochs have occurred\n",
    "warmup = 100            # number of epochs to wait before enacting early stopping policy\n",
    "patience = 0            # number of epochs with no improvement in eval metric to allow before early stopping"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Trainer and solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "    def end_epoch(self, trainer, output):\n",
    "        if trainer.current_epoch % trainer.epoch_verbose == 0:\n",
    "            train_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"train\"]]\n",
    "            dev_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"dev\"]]\n",
    "            clear_output(wait=True)\n",
    "            plt.semilogy(train_loss_history, label=\"Train loss\")\n",
    "            plt.semilogy(dev_loss_history, label=\"Dev loss\")\n",
    "            plt.xlabel(\"# Epochs\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "loss_history_callback = LossHistoryCallback()\n",
    "\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    problem.to(device),\n",
    "    train_data=train_loader,\n",
    "    dev_data=dev_loader,\n",
    "    test_data=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    logger=None,\n",
    "    callback=loss_history_callback,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    epoch_verbose=epoch_verbose,\n",
    "    train_metric='train_loss',\n",
    "    dev_metric='dev_loss',\n",
    "    test_metric='test_loss',\n",
    "    eval_metric=\"dev_loss\",\n",
    "    warmup = warmup,\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "best_model = trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load best trained model\n",
    "best_outputs = trainer.test(best_model)\n",
    "problem.load_state_dict(best_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"train\"]]\n",
    "dev_loss_history = [l.detach().cpu().numpy() for l in trainer.loss_history[\"dev\"]]\n",
    "mean_test_loss = best_outputs['mean_test_loss'].detach().cpu().numpy()\n",
    "print(mean_test_loss)\n",
    "print(f\"len(train_loss_history): {len(train_loss_history)}\")\n",
    "print(f\"len(dev_loss_history): {len(dev_loss_history)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss history w/ mean test loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.semilogy(train_loss_history, label=\"Train loss\")\n",
    "plt.semilogy(dev_loss_history, label=\"Dev loss\")\n",
    "plt.scatter(len(train_loss_history), mean_test_loss, label=\"Mean test loss\", c=\"red\", marker='x')\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "k = 18 # k is the k-th function among the 500 test functions\n",
    "v_ = test_data.datadict[\"branch_inputs\"][:,k].reshape(-1,1)\n",
    "x_ = test_data.datadict[\"trunk_inputs\"]\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = test_data.datadict[\"outputs\"][:,k]\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='v_')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='u_')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='u_est')\n",
    "\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Integral Examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_ = train_data.datadict[\"trunk_inputs\"]\n",
    "v_ = torch.pow(x_,2).reshape(-1,1)\n",
    "\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = (1./3.)*torch.pow(x_,3).reshape(-1,1)\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='$v(x) = x^2$')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='integral of v, exact ($x^3/3$)')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='integral of v, estimated')\n",
    "plt.legend()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_ = train_data.datadict[\"trunk_inputs\"]\n",
    "v_ = torch.cos(x_).reshape(-1,1)\n",
    "\n",
    "print(v_.shape, x_.shape)\n",
    "\n",
    "v_ = v_.to(device)\n",
    "x_ = x_.to(device)\n",
    "\n",
    "res = problem.predict({'branch_inputs':v_, 'trunk_inputs':x_})\n",
    "\n",
    "u_ = torch.sin(x_).reshape(-1,1)\n",
    "u_est = res['g'].T\n",
    "\n",
    "plt.plot(x_.detach().cpu().numpy(), v_.detach().cpu().numpy(),label='$v(x) = cos(x)$')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_.detach().cpu().numpy(),label='integral of v, exact ($sin(x)$)')\n",
    "plt.plot(x_.detach().cpu().numpy(), u_est.detach().cpu().numpy(),label='integral of v, estimated')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eerc-deeponet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
